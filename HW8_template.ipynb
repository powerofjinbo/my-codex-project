{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P53 Homework 8\n",
    "\n",
    "Save this notebook as `HW8_<ucinetid>.ipynb` where `<ucinetid>` is your UCInetID.  For example, `HW8_dkirkby.ipynb`.  Make sure you use this custom file name when you upload your homework to Canvas. You will upload both the notebook file (with extension `.ipynb`) and a PDF of the notebook. To create the PDF, use **Save and Export Notebook As...** from the **File** menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "\n",
    "During Lecture 17, we generated and fit data that follows a straight line, with the unknown parameters of slope and offset. In this problem, we will use the model:\n",
    "$$\n",
    "y(x) = A \\cos(2\\pi (x - x_0) / L)\n",
    "$$\n",
    "with unknown parameters $A$ (amplitude), $x_0$ (phase) and $L$ (wavelength).\n",
    "\n",
    "First, implement the function below to evaluate this model's prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oscillation_model(x, A, L, x0):\n",
    "    return A * np.cos(2 * np.pi * (x - x0) / L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will generate values of $y_i$ on a uniform grid of $x_i$ values (instead of the random $x_i$ values we used in the line fit example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata = np.linspace(0, 10, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function below to generate and return a numpy array of $y_i$ values for each $x_i$ in the input array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(x, A, L, x0, noise=0, seed=1):\n",
    "    return A * np.cos(2 * np.pi * (x - x0) / L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and plot the data with some noise. You should be able to see roughly one cycle of an oscillation if you implemented `generate_data` correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydata = generate_data(xdata, A=2, L=7, x0=1, noise=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xdata, ydata, '.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a copy the `chi_square` and `chi_square_fit` functions from Lecture 17 that we will use below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square(observed, predicted, sigma):\n",
    "    \n",
    "    pulls = (observed - predicted) / sigma\n",
    "    return pulls.dot(pulls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_fit(x, y, model, initial_params):\n",
    "    \n",
    "    def f(params):\n",
    "        prediction = model(x, *params)\n",
    "        return chi_square(y, prediction, sigma=1)\n",
    "    \n",
    "    opt = scipy.optimize.minimize(f, initial_params, method='Nelder-Mead')\n",
    "    print(opt.message)\n",
    "    return opt.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Perform the fit to obtain the parameter values that best fit the (noisy) data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Afit, Lfit, x0fit = chi_square_fit(xdata, ydata, oscillation_model, [1, 5, 0])\n",
    "print(f'Best fit: A={Afit:.2f}, L={Lfit:.2f}, x0={x0fit:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, make a plot that shows three things:\n",
    " - the model curve with the true parameter values,\n",
    " - the model curve with the best-fit parameters, and\n",
    " - the generated data used for the fit.\n",
    "\n",
    "Use the `label=` arguments and `plt.legend()` function so that each of these is labeled in your plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Instead of fitting points directly, you can also fill a histogram with some data, then fit the number of entries in each bin to a model.\n",
    "\n",
    "Start by implementing the function below to generate random values sampled from a normal (aka Gaussian) distribution with the specified mean $\\mu$ and standard deviation $\\sigma$.  Refer to Lecture 9 and Week 5 discussion for details on generating random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, mu, sigma, seed=1):\n",
    "    return A * np.cos(2 * np.pi * (x - x0) / L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 500 values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = generate_data(500, 0.5, 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the histogram bin edges to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_edges = np.linspace(-5, 5, 26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the corresponding bin centers (note that this array is shorter than `bin_edges` - think about why):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin_centers = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the bin width $\\Delta x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dx = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get an array of histogram entries in each bin (refer to the Week 9 discussion for details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_entries, _ = np.histogram(x, bins=bin_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function below to evaluate a Gaussian model:\n",
    "$$\n",
    "f(x) = \\frac{A \\Delta x}{\\sqrt{2\\pi}\\sigma}\\, \\exp\\left(\n",
    "-\\frac{(x - \\mu)^2}{2\\sigma^2}\n",
    "\\right)\n",
    "$$\n",
    "with unknown parameters $A$ (normalization), $\\mu$ (mean) and $\\sigma$ (width).  Note that this $\\sigma$ is not the same $\\sigma$ that appears in the $\\chi^2$ equation: one is a model parameter and the other is the data uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_model(bin_center, A, mu, sigma):\n",
    "    return A * np.cos(2 * np.pi * (x - x0) / L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a $\\chi^2$ fit to estimate the value of the unknown parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_fit, mu_fit, sigma_fit = chi_square_fit(bin_centers, bin_entries, gaussian_model, [100, 0, 1])\n",
    "print(f'Best fit: A = {A_fit:.2f}, mu = {mu_fit:.2f}, sigma = {sigma_fit:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the $A$ parameter approximates the total number of entries in the histogram.\n",
    "\n",
    "Finally, plot the histogram of data with two smooth curves superimposed, showing the model with the true and best-fit parameters. Add a legend with labels to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "In this problem, we return to the straight line fit but work with measurements that do not have a constant uncertainty $\\sigma_i$. Specifically, we will fit data over the range $-1 \\le x \\le +1$ with $\\sigma_i = x_i^2$.\n",
    "\n",
    "Start by implementing the function below to generate random values of $x_i$, $y_i$, along with the corresponding uncertainties $\\sigma_i$, assuming a linear relationship between $y$ and $x$.  Your function should return three arrays containing the values of $x_i$, $y_i$ and $\\sigma_i$ and the returned values of $y_i$ should include random errors, sampled according to each $\\sigma_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(npoints, slope, offset, seed=1):\n",
    "    return A * np.cos(2 * np.pi * (x - x0) / L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 20 data points with their uncertainties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, sigma = generate_data(20, 2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot this generated data using error bars to represent the uncertainties, together with a smooth curve showing the model with the true parameter vaues. Refer to the Week 9 discussion for an example of using the matplotlib [errorbar function](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.errorbar.html).  Your plot should clearly show larger error bars **and larger scatter about the line** for points near $x = \\pm 1$ if you have implemented `generate_data` correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "\n",
    "Building on the previous problem, we now fit the generated data.  First, pretend that we do not know the individual errors $\\sigma_i$ and fit assuming $\\sigma_i = \\sigma = 1$ as we did in the problems above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_model(x, slope, offset):\n",
    "    \n",
    "    return slope * x + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_no_sigmas, offset_no_sigmas = chi_square_fit(x, y, line_model, [0, 0])\n",
    "print(f'Fit with no sigmas: slope = {slope_no_sigmas:.2f}, offset = {offset_no_sigmas:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, adapt the `chi_square_fit` function above to use the known uncertainties $\\sigma_i$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_square_fit_with_sigmas(x, y, sigma, model, initial_params):\n",
    "    return A * np.cos(2 * np.pi * (x - x0) / L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit using these known uncertainties and compare the resulting fit parameters.  You should notice that this second fit gives more accurate results if you implemented `chi_square_fit_with_sigmas` correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_with_sigmas, offset_with_sigmas = chi_square_fit_with_sigmas(x, y, sigma, line_model, [0, 0])\n",
    "print(f'Fit with sigmas: slope = {slope_with_sigmas:.2f}, offset = {offset_with_sigmas:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, plot the data (using error bars) with three models superimposed:\n",
    " - the model using the true parameters\n",
    " - the model using the parameters fit with no sigmas\n",
    " - the model using the parameters fit with sigmas\n",
    "\n",
    "Add labels to your plot. If you have implemented `chi_square_fit_with_sigmas` correctly, you should find that the fit with sigmas is closer to the true model, which demonstrates the importance of including known uncertainties when they are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
